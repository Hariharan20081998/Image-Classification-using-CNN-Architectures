{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wang dataset INCEPTION_V3",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DeI_3ffbKXG"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from glob import glob\n",
        "import matplotlib.pylab as plt\n",
        "import cv2\n",
        "import sklearn\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, StratifiedKFold, learning_curve, GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix, make_scorer, accuracy_score\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential, model_from_json\n",
        "from keras.optimizers import SGD, RMSprop, Adam, Adagrad, Adadelta\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, Conv2D, MaxPool2D, MaxPooling2D\n",
        "%matplotlib inline\n",
        "import keras\n",
        "from keras import callbacks\n",
        "from keras import optimizers\n",
        "from keras.engine import Model\n",
        "from keras.layers import Dropout, Flatten, Dense\n",
        "from tensorflow.keras.layers import BatchNormalization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zL_YYqd6gw9V"
      },
      "source": [
        "# Mount Google Drive (for COLAB)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t368M_Imb3n1"
      },
      "source": [
        "# Load the Dataset\n",
        "from glob import glob\n",
        "train = glob('/content/gdrive/My Drive/wangdataset/Images/*.jpg', recursive=True)\n",
        "test = glob('/content/gdrive/My Drive/wangdataset/Images/test/*.jpg', recursive=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LA9zcJsDg_Wx"
      },
      "source": [
        "import math\n",
        "from math import floor\n",
        "x_train = []\n",
        "x_test = []\n",
        "y_train = []\n",
        "y_test = []\n",
        "WIDTH = 128\n",
        "HEIGHT = 128\n",
        "for img in train:\n",
        "    filename = os.path.basename(img)\n",
        "    label = floor( int(filename.split('.')[0]) / 100 )\n",
        "    y_train.append(label)\n",
        "  \n",
        "for img in test:\n",
        "    filename = os.path.basename(img)\n",
        "    label = floor( int(filename.split('.')[0]) / 100 )\n",
        "    y_test.append(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWIMd9IwzbeU"
      },
      "source": [
        "for img in train:\n",
        "    full_size_image = cv2.imread(img)\n",
        "    x_train.append(cv2.resize(full_size_image, (WIDTH,HEIGHT)))\n",
        "for img in test:\n",
        "    full_size_image = cv2.imread(img)\n",
        "    x_test.append(cv2.resize(full_size_image, (WIDTH,HEIGHT)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xpUm5rvCIiG"
      },
      "source": [
        "X_train= np.array(x_train)\n",
        "X_train= X_train/255.0\n",
        "X_test= np.array(x_test)\n",
        "X_test= X_test/255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4UzmlU6CFT1"
      },
      "source": [
        "# One Hot Encoding\n",
        "from keras.utils.np_utils import to_categorical\n",
        "y_trainHot = to_categorical( y_train, num_classes = 10)\n",
        "y_testHot = to_categorical( y_test, num_classes = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQMKsF3ECXlJ"
      },
      "source": [
        "# Load the Model\n",
        "base_model = keras.applications.inception_v3.InceptionV3(weights='imagenet', include_top=False, input_shape=(128,128, 3), classes=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oacjtqf0CfVV"
      },
      "source": [
        "last = base_model.get_layer('mixed10').output\n",
        "# Add classification layers on top of it\n",
        "x = BatchNormalization()\n",
        "x = Flatten()(last)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "pred = Dense(10, activation='softmax')(x)\n",
        "\n",
        "model =Model(base_model.input, pred)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Xx_r_ezDClC"
      },
      "source": [
        "# Compile and Fit the Model\n",
        "from keras.optimizers import SGD, RMSprop, Adam, Adagrad, Adadelta\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=keras.optimizers.SGD(lr=1e-3, momentum=0.9),\n",
        "              metrics=['acc'])\n",
        "a = X_train\n",
        "b = y_trainHot\n",
        "history = model.fit(a, b, epochs=200, batch_size=128, validation_split=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS8upiIRsnPV"
      },
      "source": [
        "# Metrics\n",
        "test_loss, test_acc = model.evaluate(X_test,y_testHot)\n",
        "print(\"Test Loss: \", test_loss*100)\n",
        "print(\"Test Accuracy: \", test_acc*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a62E9K0k7sWi"
      },
      "source": [
        "# Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuVk8snj8VIk"
      },
      "source": [
        "# Evaluation Metrics\n",
        "target_names = ['Tribes','Beach', 'Monuments', 'Bus', 'Dinosauras'\n",
        "           ,'Elephant', 'Flower', 'Hourse', \n",
        "            'Mountains', 'Food']\n",
        "# Prediction           \n",
        "def reports(X_test,y_test):\n",
        "    Y_pred = model.predict(X_test)\n",
        "    y_pred = np.argmax(Y_pred, axis=1)\n",
        "\n",
        "    classification = classification_report(np.argmax(y_test, axis=1), y_pred, target_names=target_names)\n",
        "    confusion = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n",
        "    score = model.evaluate(X_test, y_test, batch_size=32)\n",
        "    Test_Loss = score[0]*100\n",
        "    Test_accuracy = score[1]*100\n",
        "    kc=cohen_kappa_score(np.argmax(y_test, axis=1), y_pred)\n",
        "    #mse=mean_squared_error(y_test, y_pred)\n",
        "    #mae=mean_absolute_error(y_test, y_pred)\n",
        "    #precision=precision_score(y_test, y_pred, average='weighted')\n",
        "    #print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "\n",
        "\n",
        "    \n",
        "    return classification, confusion, Test_Loss, Test_accuracy ,kc#,mse,mae"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zP97Tkzn7wwD"
      },
      "source": [
        "#model = load_model('/content/gdrive/My Drive/wangdataset/model_hb.h5')\n",
        "from sklearn.metrics import classification_report, confusion_matrix,cohen_kappa_score\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, precision_score\n",
        "# calculate result, loss, accuray and confusion matrix\n",
        "classification, confusion, Test_loss, Test_accuracy,kc = reports(X_test,y_testHot)\n",
        "classification = str(classification)\n",
        "confusion_str = str(confusion)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_ASeTPx9I5V"
      },
      "source": [
        "print(\"confusion matrix: \")\n",
        "print('{}'.format(confusion_str))\n",
        "print(\"KAppa Coeefecient=\",kc)\n",
        "print('Test loss {} (%)'.format(Test_loss))\n",
        "print('Test accuracy {} (%)'.format(Test_accuracy))\n",
        "#print(\"Mean Squared error=\",mse)\n",
        "#print(\"Mean absolute error=\",mae)\n",
        "print(classification)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmwR10Tn9Lma"
      },
      "source": [
        "# Plot the Confusion Matrix\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.get_cmap(\"Blues\")):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    Normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    if normalize:\n",
        "        cm = Normalized\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    #print(cm)\n",
        "\n",
        "    plt.imshow(Normalized, interpolation='nearest', cmap=cmap)\n",
        "    plt.colorbar()\n",
        "    plt.title(title)\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=90)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.4f' if normalize else 'd'\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        thresh = cm[i].max() / 2.\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "plot_confusion_matrix(confusion, classes=target_names, normalize=False, \n",
        "                      title='Confusion matrix, without normalization')\n",
        "#plt.savefig(\"/content/gdrive/My Drive/wangdataset/result/confusion_matrix_without_normalization.svg\")\n",
        "plt.show()\n",
        "plt.figure(figsize=(15,15))\n",
        "plot_confusion_matrix(confusion, classes=target_names, normalize=True, \n",
        "                      title='Normalized confusion matrix')\n",
        "#plt.savefig(\"/content/gdrive/My Drive/wangdataset/result/confusion_matrix_with_normalization.png\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKgHF8GK7c-c"
      },
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.grid(True)\n",
        "plt.legend(['train', 'test'], loc='upper left') \n",
        "plt.savefig(\"/content/gdrive/My Drive/wangdataset/model_accuracy_100.svg\")\n",
        "plt.show()\n",
        "\n",
        "# summarize history for loss \n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.grid(True)\n",
        "plt.legend(['train', 'test'], loc='upper left') \n",
        "plt.savefig(\"/content/gdrive/My Drive/wangdataset/model_loss_100.svg\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}